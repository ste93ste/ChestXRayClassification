{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised Convolutional Auxiliary Deep Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from imgaug import augmenters as iaa\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "import pydicom, numpy as np\n",
    "from skimage.transform import resize\n",
    "sys.path.append(\"../semi-supervised-pytorch-master/semi-supervised\") # path to models\n",
    "det_class_path = '../Kaggle/all/stage_2_detailed_class_info.csv' # class info\n",
    "bbox_path = '../Kaggle/all/stage_2_train_labels.csv' # labels\n",
    "dicom_dir = '../Kaggle/all/stage_2_train_images/' # train images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the image-level labels for the scans. The most interesting group here is the No Lung Opacity / Not Normal since they are cases that look like opacity but are not. The classes are balanced so we don't need to cope to unbalanced classes problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_class_df = pd.read_csv(det_class_path)\n",
    "print(det_class_df.shape[0], 'class infos loaded')\n",
    "print(det_class_df['patientId'].value_counts().shape[0], 'patient cases')\n",
    "det_class_df.groupby('class').size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "def indices_to_one_hot(data, nb_classes):\n",
    "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    return np.eye(nb_classes)[targets]\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    \"\"\"Return a batch from the iterable\"\"\"\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training dataset with labelled and unlabelled images\n",
    "image_df = pd.DataFrame({'path': glob(os.path.join(dicom_dir, '*.dcm'))})\n",
    "image_df['patientId'] = image_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "\n",
    "# training/validation slit\n",
    "validation = 0.1 \n",
    "# image resize (the bigger the better, except for computational power ...)\n",
    "image_resize = 225 \n",
    "# number of unlabelled images in the training dataset\n",
    "labelled_images = 1000\n",
    "# number of labelled ones (the rest of the training dataset)\n",
    "unlabelled_images = int(image_df.shape[0]*(1-validation)-labelled_images) \n",
    "# number of validation images\n",
    "validation_images = int(validation*image_df.shape[0])\n",
    "# Some list for the dataset probably I should use something faster ...\n",
    "labelled = []\n",
    "label = []\n",
    "unlabelled = []\n",
    "# We don't need the same subject repated when multiple bounding boxes occur\n",
    "det_class_df.drop_duplicates()\n",
    "allLabel = pd.get_dummies(pd.Series(list(det_class_df['class']))).values\n",
    "label0Count = 0\n",
    "label1Count = 0\n",
    "label2Count = 0\n",
    "labelIndex = []\n",
    "finishLabelling = False\n",
    "\n",
    "# Prepare training dataset\n",
    "i = 0\n",
    "done = 0\n",
    "while(not finishLabelling):\n",
    "    if allLabel[i][0] == 1 and label0Count < labelled_images/3:\n",
    "        done += 1\n",
    "        label0Count += 1\n",
    "        labelIndex.append(i)\n",
    "        k = np.where(image_df['patientId'] == det_class_df['patientId'][i])\n",
    "        labelled.append(resize(pydicom.read_file(image_df['path'].values[k[0]][0]).pixel_array/255, \n",
    "                            (image_resize,image_resize), anti_aliasing=True, mode='constant'))\n",
    "        label.append(allLabel[i])\n",
    "    elif allLabel[i][1] == 1 and label1Count < labelled_images/3:\n",
    "        done += 1        \n",
    "        label1Count += 1\n",
    "        labelIndex.append(i)\n",
    "        k = np.where(image_df['patientId'] == det_class_df['patientId'][i])\n",
    "        labelled.append(resize(pydicom.read_file(image_df['path'].values[k[0]][0]).pixel_array/255, \n",
    "                            (image_resize,image_resize), anti_aliasing=True, mode='constant'))\n",
    "        label.append(allLabel[i])\n",
    "    elif allLabel[i][2] == 1 and label2Count < labelled_images/3:\n",
    "        done += 1\n",
    "        label2Count += 1\n",
    "        labelIndex.append(i)\n",
    "        k = np.where(image_df['patientId'] == det_class_df['patientId'][i])\n",
    "        labelled.append(resize(pydicom.read_file(image_df['path'].values[k[0]][0]).pixel_array/255, \n",
    "                            (image_resize,image_resize), anti_aliasing=True, mode='constant'))\n",
    "        label.append(allLabel[i])\n",
    "    if label0Count == labelled_images/3 and label1Count == labelled_images/3 and label2Count == labelled_images/3:\n",
    "        finishLabelling = True\n",
    "    i += 1\n",
    "    if done % 1000 == 0:\n",
    "        print(str(done) + ' labelled images out of ' + str(labelled_images) + ' done')\n",
    "\n",
    "print(str(labelled_images) + ' training images labelled loaded')\n",
    "\n",
    "done = 0\n",
    "for i in range(labelled_images + unlabelled_images):\n",
    "    if i not in labelIndex:\n",
    "        done += 1\n",
    "        labelIndex.append(i)\n",
    "        k = np.where(image_df['patientId'] == det_class_df['patientId'][i])\n",
    "        unlabelled.append(resize(pydicom.read_file(image_df['path'].values[k[0]][0]).pixel_array/255,\n",
    "                                (image_resize,image_resize), anti_aliasing=True, mode='constant'))\n",
    "        if done % 1000 == 0 and done != 0:\n",
    "            print(str(done) + ' unlabelled images out of ' + str(unlabelled_images) + ' done')\n",
    "\n",
    "print(str(unlabelled_images) + ' training images unlabelled loaded')\n",
    "\n",
    "# Prepare validation dataset\n",
    "labelled_val = []\n",
    "label_val = []\n",
    "done = 0\n",
    "for i in range(int(image_df.shape[0])):\n",
    "    if i not in labelIndex:\n",
    "        done += 1\n",
    "        label_val.append(allLabel[i])\n",
    "        k = np.where(image_df['patientId'] == det_class_df['patientId'][i])\n",
    "        labelled_val.append(resize(pydicom.read_file(image_df['path'].values[k[0]][0]).pixel_array/255, \n",
    "                        (image_resize,image_resize), anti_aliasing=True, mode='constant'))\n",
    "        if done % 1000 == 0:\n",
    "            print(str(done) + ' images out of ' + str(validation_images) + ' done')\n",
    "\n",
    "print('Validation images loaded')\n",
    "\n",
    "trainNbr = np.sum(label, axis=0)\n",
    "valNbr = np.sum(label_val, axis=0)\n",
    "\n",
    "print('Summary:')\n",
    "\n",
    "print('Training images: ' + str(labelled_images + unlabelled_images))\n",
    "print('Labelled: ' + str(labelled_images) + ', Unlabelled: ' + str(unlabelled_images))\n",
    "print('Labels: Opacity ' + str(trainNbr[0]) + ', Not-normal ' + str(trainNbr[1]) + ', Normal ' + str(trainNbr[2]))\n",
    "\n",
    "print('Validation images: ' + str(validation_images))\n",
    "print('Labels: Opacity ' + str(valNbr[0]) + ', Not-normal ' + str(valNbr[1]) + ', Normal ' + str(valNbr[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Deep Generative Model\n",
    "\n",
    "The Auxiliary Deep Generative Model [[MaalÃ¸e, 2016]](https://arxiv.org/abs/1602.05473) posits a model that with an auxiliary latent variable $a$ that infers the variables $z$ and $y$. This helps in terms of semi-supervised learning by delegating causality to their respective variables. \n",
    "\n",
    "We create the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AuxiliaryDeepGenerativeModel\n",
    "\n",
    "y_dim = 3\n",
    "z_dim = 128\n",
    "a_dim = 128\n",
    "h_dim = [2048, 1024, 512, 256]\n",
    "\n",
    "model = AuxiliaryDeepGenerativeModel([image_resize*image_resize, y_dim, z_dim, a_dim, h_dim])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from inference import SVI, DeterministicWarmup, log_gaussian\n",
    "\n",
    "# We will need to use warm-up in order to achieve good performance.\n",
    "# Over 200 calls to SVI we change the autoencoder from\n",
    "# deterministic to stochastic.\n",
    "\n",
    "def log_gauss(x, mu, var):\n",
    "    return -(log_gaussian(x, mu, var))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "beta = DeterministicWarmup(n=100)\n",
    "beta_constant = 0.1\n",
    "alpha = beta_constant * (len(unlabelled) + len(labelled)) / len(labelled)\n",
    "\n",
    "\n",
    "if cuda: model = model.cuda()\n",
    "elbo = SVI(model, likelihood=log_gauss, beta=beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library is conventially packed with the `SVI` method that does all of the work of calculating the lower bound for both labelled and unlabelled data depending on whether the label is given. It also manages to perform the enumeration of all the labels.\n",
    "\n",
    "Remember that the labels have to be in a *one-hot encoded* format in order to work with SVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "n_epochs = 100\n",
    "batchSize= 15\n",
    "\n",
    "# Some variables for plotting losses\n",
    "accuracyTrain = []\n",
    "accuracyVal = []\n",
    "LTrain = []\n",
    "LVal = []\n",
    "UTrain = []\n",
    "UVal = []\n",
    "classTrain = []\n",
    "classVal = []\n",
    "JAlphaTrain = []\n",
    "JAlphaVal = []\n",
    "image_augmenter = iaa.SomeOf((1, None),[iaa.Fliplr(0.5),\n",
    "                                        iaa.Affine(scale=(0.8, 1.2),\n",
    "                                        translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)},\n",
    "                                        rotate=(-15, 15))\n",
    "                                        ],random_order=True,)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_L_train, total_U_train, total_classification_loss_train, total_loss_train, accuracy_train = (0, 0, 0, 0, 0)\n",
    "    total_L_val, total_U_val, total_classification_loss_val, total_loss_val, accuracy_val = (0, 0, 0, 0, 0)\n",
    "    m_train, m_val = (0, 0)\n",
    "    \n",
    "    # Shuffle the data every epoch (labelled and label should keep the same index ordering!)\n",
    "    z = list(zip(labelled, label))\n",
    "    random.shuffle(z)\n",
    "    random.shuffle(unlabelled)\n",
    "    labelled, label = zip(*z)\n",
    "    latent = []\n",
    "    y_pred = []\n",
    "    for x, y, u in zip(cycle(batch(labelled, batchSize)), cycle(batch(label, batchSize)), (batch(unlabelled, batchSize))):\n",
    "        m_train+=1\n",
    "        x = image_augmenter.augment_images(x)\n",
    "        u = image_augmenter.augment_images(u)\n",
    "\n",
    "        # Wrap in variables\n",
    "        x, y, u = torch.from_numpy(np.asarray(x).reshape(-1, image_resize*image_resize)), torch.Tensor(y), torch.from_numpy(np.asarray(u).reshape(-1, image_resize*image_resize))\n",
    "        x, y, u = x.type(torch.FloatTensor), y.type(torch.FloatTensor), u.type(torch.FloatTensor)\n",
    "\n",
    "        if cuda:\n",
    "            # They need to be on the same device and be synchronized.\n",
    "            x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "            u = u.cuda(device=0)\n",
    "\n",
    "        L, _ = elbo(x, y) \n",
    "        U, _ = elbo(u)\n",
    "\n",
    "        # Add auxiliary classification loss q(y|x)\n",
    "        logits = model.classify(x)\n",
    "\n",
    "        # Regular cross entropy\n",
    "        classication_loss = - torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "        J_alpha_train = - L + alpha * classication_loss - U\n",
    "        \n",
    "        J_alpha_train.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_L_train += L.item()\n",
    "        total_U_train += U.item()\n",
    "        total_classification_loss_train += classication_loss.item()\n",
    "        total_loss_train += J_alpha_train.item()\n",
    "        accuracy_train += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "        \n",
    "    model.eval()\n",
    "    for x, y in zip(batch(labelled_val, batchSize), batch(label_val, batchSize)):\n",
    "        m_val+=1\n",
    "        x, y = torch.from_numpy(np.asarray(x).reshape(-1, image_resize*image_resize)), torch.Tensor(y)\n",
    "        x, y = x.type(torch.FloatTensor), y.type(torch.FloatTensor)\n",
    "\n",
    "        if cuda:\n",
    "            x, y = x.cuda(device=0), y.cuda(device=0)\n",
    "\n",
    "        L, z = elbo(x, y) \n",
    "        U, _ = elbo(x)\n",
    "        latent.append(z.cpu().detach().numpy())\n",
    "\n",
    "        logits = model.classify(x)\n",
    "        y_pred.append(torch.max(logits, 1)[1].cpu().detach().numpy())\n",
    "        classication_loss = - torch.sum(y * torch.log(logits + 1e-8), dim=1).mean()\n",
    "        J_alpha_val = - L + alpha * classication_loss - U\n",
    "        \n",
    "        total_L_val += L.item()\n",
    "        total_U_val += U.item()\n",
    "        total_classification_loss_val += classication_loss.item()\n",
    "        total_loss_val += J_alpha_val.item()\n",
    "        accuracy_val += torch.mean((torch.max(logits, 1)[1].data == torch.max(y, 1)[1].data).float())\n",
    "\n",
    "    print(\"Epoch: {}\".format(epoch+1))\n",
    "    print(\"[Train]\\t\\t L: {:.2f}, U: {:.2f}, class: {:.2f}, J_a: {:.2f}, accuracy: {:.2f}\".format(total_L_train / m_train, total_U_train / m_train, total_classification_loss_train / m_train, total_loss_train / m_train, accuracy_train / m_train))\n",
    "    print(\"[Validation]\\t L: {:.2f}, U: {:.2f}, class: {:.2f}, J_a: {:.2f}, accuracy: {:.2f}\".format(total_L_val / m_val, total_U_val / m_val, total_classification_loss_val / m_val, total_loss_val / m_val, accuracy_val / m_val))\n",
    "    \n",
    "    accuracyTrain.append(accuracy_train / m_train)\n",
    "    accuracyVal.append(accuracy_val / m_val)\n",
    "    LTrain.append(total_L_train / m_train)\n",
    "    LVal.append(total_L_val / m_val)\n",
    "    UTrain.append(total_U_train / m_train)\n",
    "    UVal.append(total_U_val / m_val)\n",
    "    classTrain.append(total_classification_loss_train / m_train)\n",
    "    classVal.append(total_classification_loss_val / m_val)\n",
    "    JAlphaTrain.append(total_loss_train / m_train)\n",
    "    JAlphaVal.append(total_loss_val / m_val)\n",
    "    \n",
    "    plt.figure(1, figsize=(20, 20))\n",
    "    plt.subplot(321)\n",
    "    plt.plot(accuracyTrain, 'r', label='train acc')\n",
    "    plt.plot(accuracyVal, 'b', label='val acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(322)\n",
    "    plt.plot(classTrain, 'r', label='train class')\n",
    "    plt.plot(classVal, 'b', label='val class')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Classification')\n",
    "    plt.legend()\n",
    "    plt.subplot(323)\n",
    "    plt.plot(LTrain, 'r', label='train L')\n",
    "    plt.plot(LVal, 'b', label='val L')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('L')\n",
    "    plt.legend()\n",
    "    plt.subplot(324)\n",
    "    plt.plot(UTrain, 'r', label='train U')\n",
    "    plt.plot(UVal, 'b', label='val U')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('U')\n",
    "    plt.legend()\n",
    "    plt.subplot(325)\n",
    "    plt.plot(JAlphaTrain, 'r', label='train J-alpha')\n",
    "    plt.plot(JAlphaVal, 'b', label='val J-alpha')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('J-alpha')\n",
    "    plt.legend()\n",
    "    #Uncomment for latent space visualization\n",
    "    #plt.subplot(326)\n",
    "    #plt.title(\"Latent space: R:Opacity, G:Not-Normal, B:Normal\")\n",
    "    #plt.xlabel('Dimension 1')\n",
    "    #plt.ylabel('Dimension 2')\n",
    "    #latent = np.vstack(latent)\n",
    "    #latent = np.array(latent, dtype=np.float32).reshape(-1, z_dim)\n",
    "    #latent = PCA(n_components=2).fit_transform(latent)\n",
    "    #classes = np.argmax(label_val, axis=1)\n",
    "    #k = 0\n",
    "    #for z in latent:\n",
    "    #    if (classes[k] == 0):\n",
    "    #        plt.scatter(z[0], z[1], c='red', marker='o')\n",
    "    #    elif (classes[k] == 1):\n",
    "    #        plt.scatter(z[0], z[1], c='green', marker='o')\n",
    "    #    elif (classes[k] == 2):\n",
    "    #        plt.scatter(z[0], z[1], c='blue', marker='o')\n",
    "    #    k = k + 1    \n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate( y_pred, axis=0 )\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(classes, np.vstack(y_pred[0:classes.shape[0]]))\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional sampling\n",
    "We now create some samples from the model given the class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "model.eval()\n",
    "\n",
    "z = Variable(torch.randn(2, z_dim))\n",
    "z = z.cuda()\n",
    "classValue = 0\n",
    "y = torch.Tensor(indices_to_one_hot([classValue, classValue],y_dim))\n",
    "y = y.cuda()\n",
    "x_mu, x_log_var = model.sample(z, y)\n",
    "norm = Normal(x_mu,x_log_var)\n",
    "x = norm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(40, 40))\n",
    "\n",
    "samples = x.data.view(-1, image_resize, image_resize).cpu().numpy()\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(samples[i], cmap='bone')\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHMODEL = '/home/stce/Scaricati/Unumed/Models/beta01'\n",
    "PATHFIGURE = '/home/stce/Scaricati/Unumed/Figure/training.npz'\n",
    "torch.save(model.state_dict(), PATHMODEL)\n",
    "torch.save(model.state_dict(), PATHMODEL)\n",
    "np.savez(PATHFIGURE,accuracyTrain=accuracyTrain,\n",
    "accuracyVal=accuracyVal,\n",
    "classTrain=classTrain,\n",
    "classVal=classVal,\n",
    "LTrain=LTrain,\n",
    "LVal=LVal,\n",
    "UTrain=UTrain,\n",
    "UVal=UVal,\n",
    "JAlphaTrain=JAlphaTrain,\n",
    "JAlphaVal=JAlphaVal,\n",
    "conf_matrix=conf_matrix,\n",
    "latent=latent,\n",
    "classes=classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
